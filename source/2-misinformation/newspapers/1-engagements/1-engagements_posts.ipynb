{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_misinformation():\n",
    "    misinformations = pd.read_excel('/Users/cblanesg/misinformation_socialmedia/data/2-misinformation/newspapers/0-misinformation/misinformation_newspapers.xlsx').drop('Unnamed: 0', axis = 1)\n",
    "    return(misinformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engagements():\n",
    "    path = '../../../../data/2-misinformation/newspapers/1-engagements/1-scrapes/'\n",
    "    data = []\n",
    "    for i in listdir(path):\n",
    "        try:\n",
    "            data.append(pd.read_json(path + i))\n",
    "        except:\n",
    "            print(i)\n",
    "    data = pd.concat(data)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_missing_desinformations():\n",
    "    misinformations = load_misinformation()\n",
    "    engagements = load_engagements()\n",
    "    \n",
    "    missinf = misinformations[~misinformations['id_desinformacion'].isin(engagements.id_desinformacion)]\n",
    "    return(missinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_engagements():\n",
    "    engagements = load_engagements()\n",
    "    misinformations = load_misinformation()\n",
    "    \n",
    "    engagements['date_publication'] = engagements['date'].apply(lambda x: x.date())\n",
    "    engagements = engagements[['id_desinformacion',\n",
    "                               'id',\n",
    "                               'date_publication',\n",
    "                               'history',\n",
    "                               'postUrl']].rename(columns = {'id':'id_post_desinformacion', \n",
    "                                             'postUrl':'link_post_desinformacion'})\n",
    "    \n",
    "    all_data = pd.merge(left = engagements, \n",
    "            right = misinformations, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left')\n",
    "    return(all_data)\n",
    "\n",
    "def add_missing_timesteps(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row =[None, \n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None,\n",
    "             None]\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            row.append(timestep)\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to obtain growth virality timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "newspapers_link_checker.json\n"
     ]
    }
   ],
   "source": [
    "missing_data = obtain_missing_desinformations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1448, 1983)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_data),len(load_misinformation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_dashboard = 'V6lMawCCbCP2rShqtU3TmYCY3Em1Osd8I2DLYxwr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [16:46<00:00,  5.08s/it] \n"
     ]
    }
   ],
   "source": [
    "for i, id_ in tqdm(zip(list(missing_data.link_desinformacion)[1250:], \n",
    "                      list(missing_data.id_desinformacion)[1250:]), total = len(list(missing_data.id_desinformacion)[1250:])):\n",
    "    URL_BASE = \"https://api.crowdtangle.com/links\"\n",
    "    PARAMS = {'link': i, 'count': 1000,'token': api_dashboard, 'platforms': 'facebook',\n",
    "         'includeHistory':'true'}\n",
    "\n",
    "    r = requests.get(url = URL_BASE, params=PARAMS)\n",
    "    data = r.json()\n",
    "    out = data['result']['posts']\n",
    "    if len(out) >= 1:\n",
    "        df = pd.DataFrame(out)\n",
    "        df['id_desinformacion'] = id_\n",
    "        posts.append(df)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.concat(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_json('../../../../data/2-misinformation/newspapers/1-engagements/1-scrapes/missing_misinformations.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Again Engagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "newspapers_link_checker.json\n"
     ]
    }
   ],
   "source": [
    "missing_data = obtain_missing_desinformations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "newspapers_link_checker.json\n"
     ]
    }
   ],
   "source": [
    "misinformations = load_misinformation()\n",
    "engagements = load_engagements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 330/69384 [01:00<4:57:02,  3.87it/s]"
     ]
    }
   ],
   "source": [
    "engagements = engagements.reset_index().drop('index', axis = 1)\n",
    "\n",
    "for i in tqdm(range(0, len(engagements))):\n",
    "    try:\n",
    "        post = engagements.loc[i]\n",
    "        if pd.isna(post['date_factcheck_final']):\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            df_history = pd.DataFrame(post['history'])\n",
    "            df_history['date_timestep'] = df_history['date'].apply(lambda x: datetime.strptime(x.split(' ')[0], '%Y-%m-%d').date())\n",
    "            df_history = df_history.reset_index().drop('index', axis = 1)\n",
    "\n",
    "            df_eng_post = []\n",
    "            for h in range(0, len(df_history)):\n",
    "                columns = list(df_history.loc[h]['actual'].keys())\n",
    "                values = list(df_history.loc[h]['actual'].values())\n",
    "                df_temp = pd.DataFrame([values], columns = columns)\n",
    "                df_temp['date_timestep'] = df_history.loc[h]['date_timestep']\n",
    "                df_temp['id_desinformacion'] = engagements.loc[i]['id_desinformacion']\n",
    "                df_temp = df_temp.drop_duplicates('date_timestep', keep = 'first')\n",
    "                df_temp['date_publication'] = post['date_publication']\n",
    "                df_temp['days_since_publication'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "                df_temp['date_factcheck'] = datetime.strptime(post['date_factcheck_final'], '%Y-%m-%d').date()\n",
    "                df_temp['days_since_factcheck'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "                df_temp['id_post_desinformacion'] = post['id_post_desinformacion']\n",
    "                df_temp['id_factcheck'] = post['id_factcheck']\n",
    "                df_temp['link_desinformacion'] = post['link_desinformacion']\n",
    "                df_temp['facebook_partnership_date'] = post['facebook_partnership_date']\n",
    "                df_temp['organizacion'] = post['organizacion']\n",
    "                df_temp['pais'] = post['pais']\n",
    "                df_eng_post.append(df_temp)\n",
    "            temp = pd.concat(df_eng_post).drop_duplicates('days_since_publication', keep = 'first')\n",
    "            panel = add_missing_timesteps(temp)\n",
    "            df_all.append(panel)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_aggregated = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "pyreadr.write_rdata(\"/Users/cblanesg/misinformation_socialmedia/data/2-misinformation/newspapers/3-clean_misinformation/newspapers.RData\", df_final_aggregated, df_name=\"misinformation_newspapers_panel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
