{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fact_checks():\n",
    "    fc = pd.read_json('../../../../data/1-factchecks/2-clean_factchecks/factchecks_data.json')\n",
    "    fc = fc[fc['type_factcheck'] == 'social_media']\n",
    "    fc\n",
    "    return(fc)\n",
    "\n",
    "def clean_label(x):\n",
    "    if x in ['Incorrect','Understated, exaggerated and incorrect',\n",
    "            'Seven incorrect, three correct, three unproven and one mostly correct']:\n",
    "        return('fake')\n",
    "    elif x in ['Mostly correct',\n",
    "               'A range of verdicts ranging from misleading to unproven and correct',\n",
    "              'Checked', 'Misleading']:\n",
    "        return('misleading')\n",
    "    else:\n",
    "        return('true')\n",
    "def load_africa_data():\n",
    "    africa = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/social/0-prepare_data/facebook_africa.xlsx')[['clean_link', \n",
    "    'id_post', \n",
    "    'id_desinformacion', \n",
    "    'Verdict']].rename(columns = {'clean_link':'link_desinformacion'})\n",
    "    africa['link_post'] = africa.link_desinformacion\n",
    "    africa['label_desinformacion'] = africa['Verdict'].apply(lambda x: clean_label(x))\n",
    "    return(africa)\n",
    "def load_social_data():\n",
    "    social = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/2-id_data/virality_fake_posts.xlsx')\n",
    "\n",
    "    social_misinformation = social[['id_post', 'id_desinformacion', 'link_desinformacion', 'link_crowdtangle', \n",
    "       'label_desinformacion', 'id_chequeo']].rename(columns = {'link_crowdtangle':'link_post', \n",
    "                                                               'id_chequeo':'id_factcheck'})\n",
    "    \n",
    "    return(social_misinformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtian_statistics_virality(df_crowd):\n",
    "    date_post = df_crowd.columns[-2:][0]\n",
    "    \n",
    "    reactions = df_crowd[['ID', 'Score Date (GMT)', 'Timestep', \n",
    "               'Likes', 'Comments', 'Shares', 'Loves', 'Wows', 'Hahas', 'Sads', 'Angrys', 'Cares', 'Reactions', 'Total Views', 'Overperforming(+)/Underperforming(-) Score']]\n",
    "    \n",
    "    link_crowdtangle = df_crowd.columns[-1:][0]\n",
    "    user_desinformacion = re.sub('https://www.facebook.com/', '', link_crowdtangle).split('/')[0]\n",
    "    link_user_desinformacion = 'https://www.facebook.com/' + user_desinformacion\n",
    "    \n",
    "    reactions['date_post'] = date_post\n",
    "    reactions['link_crowdtangle'] = link_crowdtangle\n",
    "    reactions['user_desinformacion'] = user_desinformacion\n",
    "    reactions['link_user_desinformacion'] = link_user_desinformacion\n",
    "    \n",
    "    return(reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_virality_timeseries(data_id):\n",
    "    path_crowdtangle = '/Users/cblanesg/Desktop/timeline_crowdtangle/'\n",
    "\n",
    "    id_engage = []\n",
    "    file = []\n",
    "    for i in listdir(path_crowdtangle):\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        else:\n",
    "            id_engage.append(re.sub(' [(]1[)]', '', re.sub('.csv', '', i.split('_')[-1:][0])))\n",
    "            file.append(i)\n",
    "\n",
    "    ids_posts = []\n",
    "    path_file = []\n",
    "    id_desinformacion = []\n",
    "    for i, x in zip(id_engage, file):\n",
    "        if i in list(data_id.id_post):\n",
    "            ids_posts.append(i)\n",
    "            id_desinformacion.append(list(id_data[id_data['id_post'] == i].id_desinformacion)[0])\n",
    "            path_file.append(x)\n",
    "        else:\n",
    "            for j in list(data_id.link_desinformacion):\n",
    "                if any(re.findall(i, j)):\n",
    "                    ids_posts.append(i)\n",
    "                    id_desinformacion.append(list(id_data[id_data['link_desinformacion'] == j].id_desinformacion)[0])\n",
    "                    path_file.append(x)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "    matching_data = pd.DataFrame({'id_post':ids_posts, \n",
    "                                 'id_desinformacion':id_desinformacion, \n",
    "                                 'name_file':path_file})\n",
    "    return(matching_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeid(link, id_post):\n",
    "    if any(re.findall('permalink[.]php[?]story', link)):\n",
    "        id_clean = link.split('story_fbid=')[1].split('&id=')[0]\n",
    "        return(id_clean)\n",
    "    else:\n",
    "        return(id_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date_publication(x):\n",
    "    str_date = x.split(' ')[0]\n",
    "    return(datetime.date(int(x.split(' ')[0].split('-')[0]), \n",
    "                         int(x.split(' ')[0].split('-')[1]),\n",
    "                        int(x.split(' ')[0].split('-')[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_timesteps(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row = []\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            \n",
    "            \n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append(timestep)\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.link_user_desinformacion)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(None)\n",
    "            row.append(None)\n",
    "            row.append(None)\n",
    "            row.append(None)\n",
    "            \n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data And Apply Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/0-prepare_data/id_data_virality5.xlsx')\n",
    "id_data['id_post'] = id_data.apply(lambda x: changeid(x['link_desinformacion'], x['id_post']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_engagement = match_virality_timeseries(id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_desinformacion = []\n",
    "\n",
    "for i in tqdm(range(0, len(existing_engagement))):\n",
    "    path = '/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/1-engagement/facebook/timeline_crowdtangle/'\n",
    "\n",
    "    inner_panel = obtian_statistics_virality(pd.read_csv(path + existing_engagement.loc[i]['name_file']))\n",
    "    post = existing_engagement.loc[i]['id_post']\n",
    "    id_desinformacion = existing_engagement.loc[i]['id_desinformacion']\n",
    "    file_name = existing_engagement.loc[i]['name_file']\n",
    "    \n",
    "    inner_panel['file_name'] = file_name\n",
    "    inner_panel['id_desinformacion'] = id_desinformacion\n",
    "    inner_panel['post'] = post\n",
    "    \n",
    "    timeseries_desinformacion.append(inner_panel)\n",
    "\n",
    "timeseries_desinformacion = pd.concat(timeseries_desinformacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/3-panel_disaggreagted/panel_facebook_posts.json', 'w') as f:\n",
    "    json.dump(timeseries_desinformacion.to_dict('records'), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Panel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/3-panel_disaggreagted/panel_facebook_posts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "factchecks_id = load_social_data()[['id_desinformacion', 'id_factcheck']].dropna().drop_duplicates()\n",
    "factchecks = load_data_fact_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_fc = pd.merge(left = factchecks, \n",
    "        right = factchecks_id, \n",
    "        on = 'id_factcheck', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['date_publication'] = data['date_post'].apply(lambda x: clean_date_publication(x))\n",
    "data['date_timestep'] = data['Score Date (GMT)'].apply(lambda x: clean_date_publication(x))\n",
    "data['days_since_publication'] = data.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "social = pd.merge(left = data, \n",
    "        right = complete_fc, \n",
    "        on = 'id_desinformacion', \n",
    "        how = 'left')\n",
    "social = social[~social['date_factcheck_final'].isna()]\n",
    "social['date_factcheck'] = social['date_factcheck_final'].apply(lambda x: datetime.date(int(x.split('-')[0]), \n",
    "                                                                               int(x.split('-')[1]), \n",
    "                                                                               int(x.split('-')[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "social['days_since_factcheck'] = social.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "social['loveCount'] = pd.to_numeric(social.Loves)\n",
    "social['hahaCount'] = pd.to_numeric(social.Wows)\n",
    "social['wowCount'] = pd.to_numeric(social.Hahas)\n",
    "social['sadCount'] = pd.to_numeric(social.Sads)\n",
    "social['angryCount'] = pd.to_numeric(social.Angrys)\n",
    "social['thankfulCount'] = pd.to_numeric(social.Cares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "social['reactions'] = social.apply(lambda x:\n",
    "                                      x['Loves'] + x['Wows'] + x['Hahas'] +\n",
    "                                      x['Sads'] + x['Angrys'] + \n",
    "                                      x['Cares'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_panel = social[['ID', \n",
    "       'id_desinformacion', \n",
    "        'date_publication', \n",
    "        'date_timestep', \n",
    "       'days_since_publication' ,\n",
    "                      'date_factcheck', \n",
    "                      'days_since_factcheck',\n",
    "       'facebook_partnership_date', \n",
    "       'link_user_desinformacion', \n",
    "       'link_crowdtangle',\n",
    "       'organizacion', \n",
    "       'pais', 'id_factcheck',\n",
    "                      'Likes', \n",
    "                     'Comments', 'Shares', \n",
    "                     'reactions']].rename(columns = {'ID':'id_post_desinformacion', \n",
    "                                             'link_crowdtangle':'link_desinformacion', \n",
    "                                                    'Likes':'likes', \n",
    "                                                    'Comments':'comments', \n",
    "                                                    'Shares':'shares'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Panel And interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:30<00:00, 10.09it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(input_panel.id_post_desinformacion.unique()):\n",
    "    temp = input_panel[input_panel['id_post_desinformacion'] == i]\n",
    "    temp = temp.sort_values('days_since_publication')\n",
    "    temp = temp.drop_duplicates('days_since_publication', keep= 'last')\n",
    "    df_all.append(add_missing_timesteps(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_aggregated = pd.concat(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_input = df_final_aggregated[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "       'days_since_publication', 'date_factcheck', 'days_since_factcheck',\n",
    "       'id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "       'facebook_partnership_date', 'organizacion', 'pais', \n",
    "                                'likes', 'shares', 'comments', 'reactions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:01<00:00, 285.93it/s]\n"
     ]
    }
   ],
   "source": [
    "panel_interpolate = []\n",
    "for i in tqdm(panel_input.id_post_desinformacion.unique()):\n",
    "    df = panel_input[panel_input['id_post_desinformacion'] == i].sort_values('days_since_publication')\n",
    "    df['approx_likes'] = df.likes.interpolate()\n",
    "    df['approx_shares'] = df.shares.interpolate()\n",
    "    df['approx_comments'] = df.comments.interpolate()\n",
    "    df['approx_reactions'] = df.reactions.interpolate()\n",
    "    panel_interpolate.append(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolate = pd.concat(panel_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_interpolate['approx_reactions'] = pd.to_numeric(df_interpolate.approx_reactions)\n",
    "df_interpolate['approx_likes'] = pd.to_numeric(df_interpolate.approx_likes)\n",
    "df_interpolate['approx_shares'] = pd.to_numeric(df_interpolate.approx_shares)\n",
    "df_interpolate['approx_comments'] = pd.to_numeric(df_interpolate.approx_comments)\n",
    "#df_interpolate['approx_interactions'] = pd.to_numeric(df_interpolate.approx_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_interpolate['approx_interactions'] = df_interpolate.apply(lambda x: x['approx_likes'] + \n",
    "                                                            x['approx_comments'] + x['approx_shares'] + \n",
    "                                                            x['approx_reactions'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyreadr.write_rdata('../../../../data/4-panel_data/social/panel_interpolate.RData', df_interpolate, df_name=\"misinformation_social_panel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
