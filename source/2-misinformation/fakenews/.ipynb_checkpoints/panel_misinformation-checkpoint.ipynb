{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import uuid\n",
    "from datetime import date\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_factcheck(x, y, z):\n",
    "    return(datetime.date(x, y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_africa_data():\n",
    "    africa = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/input_panel_africa.xlsx')\n",
    "    links = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/fact_checks/fact_checks_clean.xlsx')[['Link', \n",
    "'id_desinformacion']].drop_duplicates().rename(columns = {'Link':'link_desinformacion'})\n",
    "    all_data = pd.merge(left = africa, \n",
    "            right = links, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left')\n",
    "    return(all_data)\n",
    "\n",
    "def load_factchecks():\n",
    "    factchecks = pd.read_excel('../../../data/1-factchecks/2-clean_factchecks/subset_factchecks_data.xlsx')\n",
    "    factchecks['date_factcheck'] = factchecks.apply(lambda x: datetime.date(x['year'],\n",
    "                                                                            x['month'], \n",
    "                                                                            x['day']), axis = 1)\n",
    "    return(factchecks)\n",
    "\n",
    "def dates_function(x):\n",
    "    try:\n",
    "        if pd.isna(x) or  type(x) is str:\n",
    "            return(None)\n",
    "        else:\n",
    "            return(x.strftime('%Y-%m-%d'))\n",
    "    except:\n",
    "        return(x)\n",
    "\n",
    "def load_misinformation_newspapers():\n",
    "    path = '/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/04-fake_news_dataset/newspapers/Fact-Checkers/'\n",
    "    data = []\n",
    "    for i in listdir(path):\n",
    "        try:\n",
    "            data.append(pd.read_excel(path + i))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    misinformation = pd.concat(data)\n",
    "    misinformation = misinformation[[ 'id_desinformacion', 'fecha_desinformacion', \n",
    "               'link_desinformacion',  'label_desinformacion', \n",
    "                                    'id_chequeo']].drop_duplicates().rename(columns = {'id_chequeo':'id_factcheck'})\n",
    "    \n",
    "    ## include africa\n",
    "    africa = load_africa_data()\n",
    "    africa['fecha_desinformacion'] = None\n",
    "    africa = africa[list(misinformation.columns)]\n",
    "    \n",
    "    all_data = pd.concat([misinformation, africa])\n",
    "    all_data = all_data[~all_data['link_desinformacion'].isna()]\n",
    "    \n",
    "    factchecks = load_factchecks().drop(['Unnamed: 0'], axis = 1).rename(columns = {'link_facebook_factcheck':'link_post_factcheck', \n",
    "                                                                                   'date_factcheck_facebook':'date_post_factcheck'})\n",
    "    factchecks = factchecks[factchecks['type_factcheck'] == 'medios_tradicionales']\n",
    "    all_data = pd.merge(left = all_data, \n",
    "            right = factchecks, \n",
    "            on = 'id_factcheck', \n",
    "            how = 'left')\n",
    "    \n",
    "    all_data = all_data[~all_data['id_factcheck'].isna()]\n",
    "    all_data = all_data[~all_data['label_desinformacion'].isna()]\n",
    "    all_data['label_desinformacion'] = np.where(all_data['label_desinformacion'] == 'true ', 'true', all_data['label_desinformacion'])\n",
    "    all_data = all_data.drop(['fecha_desinformacion'], axis = 1)\n",
    "    return(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_africa_data_social():\n",
    "    africa = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/social/0-prepare_data/facebook_africa.xlsx')[['clean_link', \n",
    "    'id_post', \n",
    "    'id_desinformacion', \n",
    "    'Verdict']].rename(columns = {'clean_link':'link_desinformacion'})\n",
    "    africa['link_post'] = africa.link_desinformacion\n",
    "    africa['label_desinformacion'] = africa['Verdict'].apply(lambda x: clean_label_social(x))\n",
    "    return(africa)\n",
    "\n",
    "def clean_label_social(x):\n",
    "    if x in ['Incorrect','Understated, exaggerated and incorrect',\n",
    "            'Seven incorrect, three correct, three unproven and one mostly correct']:\n",
    "        return('fake')\n",
    "    elif x in ['Mostly correct',\n",
    "               'A range of verdicts ranging from misleading to unproven and correct',\n",
    "              'Checked', 'Misleading']:\n",
    "        return('misleading')\n",
    "    else:\n",
    "        return('true')\n",
    "\n",
    "def load_social_data():\n",
    "    social = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/2-id_data/virality_fake_posts.xlsx')\n",
    "\n",
    "    social_misinformation = social[['id_post', 'id_desinformacion',  'link_crowdtangle', \n",
    "       'label_desinformacion']].rename(columns = {'link_crowdtangle':'link_desinformacion'})\n",
    "    \n",
    "    africa = load_africa_data_social()\n",
    "    africa = africa[list(social_misinformation.columns)]\n",
    "    all_social = pd.concat([social_misinformation, africa])\n",
    "    \n",
    "    factchecks = load_factchecks().drop(['Unnamed: 0'], axis = 1).rename(columns = {'link_facebook_factcheck':'link_post_factcheck', \n",
    "                                                                                   'date_factcheck_facebook':'date_post_factcheck'})\n",
    "    factchecks = factchecks[factchecks['type_factcheck'] == 'social_media']\n",
    "    \n",
    "    social_id = pd.read_excel('../../../data/1-factchecks/2-clean_factchecks/factchecks_subset_social.xlsx').dropna()\n",
    "    social_id = social_id[['id_desinformacion', 'id_factcheck']].drop_duplicates()\n",
    "    \n",
    "    factchecks = pd.merge(left = factchecks, \n",
    "            right = social_id, \n",
    "            on = 'id_factcheck', how = 'left')\n",
    "    factchecks = factchecks[~factchecks['id_desinformacion'].isna()]\n",
    "    \n",
    "    all_data = pd.merge(left = all_social, \n",
    "            right = factchecks, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left').drop_duplicates()\n",
    "    \n",
    "    \n",
    "    all_data = all_data[~all_data['id_factcheck'].isna()]\n",
    "    return(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions Engagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engagements_media():\n",
    "    path = '../../../data/2-misinformation/newspapers/1-engagements/1-scrapes/'\n",
    "    data = []\n",
    "    for i in listdir(path):\n",
    "        try:\n",
    "            data.append(pd.read_json(path + i))\n",
    "        except:\n",
    "            print(i)\n",
    "    data = pd.concat(data)\n",
    "    return(data)\n",
    "\n",
    "def clean_engagements_media(misinformations):\n",
    "    engagements = load_engagements_media()\n",
    "    \n",
    "    engagements['date_publication'] = engagements['date'].apply(lambda x: x.date())\n",
    "    engagements = engagements[['id_desinformacion',\n",
    "                               'id',\n",
    "                               'date_publication',\n",
    "                               'history',\n",
    "                               'postUrl']].rename(columns = {'id':'id_post_desinformacion', \n",
    "                                             'postUrl':'link_post_desinformacion'})\n",
    "    \n",
    "    all_data = pd.merge(left = engagements, \n",
    "            right = misinformations, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left')\n",
    "    return(all_data)\n",
    "\n",
    "def add_missing_timesteps_media(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row =[np.nan, \n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan]\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            row.append(timestep)\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)\n",
    "\n",
    "\n",
    "def make_panel_media(engagements):\n",
    "    engagements = engagements[~engagements['date_factcheck'].isna()]\n",
    "    engagements = engagements.reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    df_all = []\n",
    "\n",
    "    for i in tqdm(range(0, len(engagements))):\n",
    "        try:\n",
    "            post = engagements.loc[i]\n",
    "            if pd.isna(post['date_factcheck']):\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                df_history = pd.DataFrame(post['history'])\n",
    "                df_history['date_timestep'] = df_history['date'].apply(lambda x: datetime.strptime(x.split(' ')[0], '%Y-%m-%d').date())\n",
    "                df_history = df_history.reset_index().drop('index', axis = 1)\n",
    "\n",
    "                df_eng_post = []\n",
    "                for h in range(0, len(df_history)):\n",
    "                    columns = list(df_history.loc[h]['actual'].keys())\n",
    "                    values = list(df_history.loc[h]['actual'].values())\n",
    "                    df_temp = pd.DataFrame([values], columns = columns)\n",
    "                    df_temp['date_timestep'] = df_history.loc[h]['date_timestep']\n",
    "                    df_temp['id_desinformacion'] = engagements.loc[i]['id_desinformacion']\n",
    "                    df_temp = df_temp.drop_duplicates('date_timestep', keep = 'first')\n",
    "                    df_temp['date_publication'] = post['date_publication']\n",
    "                    df_temp['days_since_publication'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "                    df_temp['date_factcheck'] = post['date_factcheck']\n",
    "                    df_temp['days_since_factcheck'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "                    df_temp['id_post_desinformacion'] = post['id_post_desinformacion']\n",
    "                    df_temp['id_factcheck'] = post['id_factcheck']\n",
    "                    df_temp['link_desinformacion'] = post['link_desinformacion']\n",
    "                    df_temp['facebook_partnership_date'] = post['facebook_partnership_date']\n",
    "                    df_temp['organizacion'] = post['organizacion']\n",
    "                    df_temp['pais'] = post['pais']\n",
    "                    df_eng_post.append(df_temp)\n",
    "                temp = pd.concat(df_eng_post).drop_duplicates('days_since_publication', keep = 'first')\n",
    "                panel = add_missing_timesteps_media(temp)\n",
    "                df_all.append(panel)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    df_final_aggregated = pd.concat(df_all)\n",
    "    return(df_final_aggregated)\n",
    "\n",
    "\n",
    "def interpolate_function(panel):\n",
    "    misinformation_newspapers_panel  = panel.sort_values(['id_post_desinformacion','days_since_publication']).reset_index()#.drop('rownames', axis = 1)\n",
    "    misinformation_newspapers_panel['loveCount'] = pd.to_numeric(misinformation_newspapers_panel.loveCount)\n",
    "    misinformation_newspapers_panel['hahaCount'] = pd.to_numeric(misinformation_newspapers_panel.hahaCount)\n",
    "    misinformation_newspapers_panel['wowCount'] = pd.to_numeric(misinformation_newspapers_panel.wowCount)\n",
    "    misinformation_newspapers_panel['sadCount'] = pd.to_numeric(misinformation_newspapers_panel.sadCount)\n",
    "    misinformation_newspapers_panel['angryCount'] = pd.to_numeric(misinformation_newspapers_panel.angryCount)\n",
    "    misinformation_newspapers_panel['thankfulCount'] = pd.to_numeric(misinformation_newspapers_panel.thankfulCount)\n",
    "    misinformation_newspapers_panel['careCount'] = pd.to_numeric(misinformation_newspapers_panel.careCount)\n",
    "    \n",
    "    misinformation_newspapers_panel['reactions'] = misinformation_newspapers_panel.apply(lambda x: x['loveCount'] + x['hahaCount'] + x['wowCount'] + x['sadCount'] + x['angryCount'] + x['thankfulCount'] + x['careCount'], axis = 1)\n",
    "    misinformation_newspapers_panel['likes'] = pd.to_numeric(misinformation_newspapers_panel.likeCount)\n",
    "    misinformation_newspapers_panel['shares'] = pd.to_numeric(misinformation_newspapers_panel.shareCount)\n",
    "    misinformation_newspapers_panel['comments'] = pd.to_numeric(misinformation_newspapers_panel.commentCount)\n",
    "    \n",
    "    panel_input = misinformation_newspapers_panel[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "                                                   'days_since_publication', 'date_factcheck', 'days_since_factcheck','id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "                                                   'facebook_partnership_date', 'organizacion', 'pais', \n",
    "                                                   'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_input = panel_input.sort_values(['id_post_desinformacion','days_since_publication'])\n",
    "    \n",
    "    \n",
    "    panel_interpolate = []\n",
    "    for i in tqdm(panel_input.id_post_desinformacion.unique()):\n",
    "        df = panel_input[panel_input['id_post_desinformacion'] == i].sort_values('days_since_publication')\n",
    "        df['approx_likes'] = df.likes.interpolate()\n",
    "        df['approx_shares'] = df.shares.interpolate()\n",
    "        df['approx_comments'] = df.comments.interpolate()\n",
    "        df['approx_reactions'] = df.reactions.interpolate()\n",
    "        df['approx_interactions'] = df.apply(lambda x: x['approx_likes'] + x['approx_shares'] + x['approx_comments'] + x['approx_reactions'], axis = 1)\n",
    "        panel_interpolate.append(df)   \n",
    "        \n",
    "    df_interpolate = pd.concat(panel_interpolate)\n",
    "    df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]\n",
    "    \n",
    "    \n",
    "    df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "    df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "    df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()\n",
    "    df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "    df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()\n",
    "    return(df_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_timesteps_social(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row = []\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            \n",
    "            \n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append(timestep)\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.link_user_desinformacion)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            \n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)\n",
    "\n",
    "def changeid(link, id_post):\n",
    "    if any(re.findall('permalink[.]php[?]story', link)):\n",
    "        id_clean = link.split('story_fbid=')[1].split('&id=')[0]\n",
    "        return(id_clean)\n",
    "    else:\n",
    "        return(id_post)\n",
    "    \n",
    "def clean_date_publication(x):\n",
    "    str_date = x.split(' ')[0]\n",
    "    return(datetime.date(int(x.split(' ')[0].split('-')[0]), \n",
    "                         int(x.split(' ')[0].split('-')[1]),\n",
    "                        int(x.split(' ')[0].split('-')[2])))    \n",
    "\n",
    "\n",
    "def obtian_statistics_virality(df_crowd):\n",
    "    date_post = df_crowd.columns[-2:][0]\n",
    "    \n",
    "    reactions = df_crowd[['ID', 'Score Date (GMT)', 'Timestep', \n",
    "               'Likes', 'Comments', 'Shares', 'Loves', 'Wows', 'Hahas', 'Sads', 'Angrys', 'Cares', 'Reactions', 'Total Views', 'Overperforming(+)/Underperforming(-) Score']]\n",
    "    \n",
    "    link_crowdtangle = df_crowd.columns[-1:][0]\n",
    "    user_desinformacion = re.sub('https://www.facebook.com/', '', link_crowdtangle).split('/')[0]\n",
    "    link_user_desinformacion = 'https://www.facebook.com/' + user_desinformacion\n",
    "    \n",
    "    reactions['date_post'] = date_post\n",
    "    reactions['link_crowdtangle'] = link_crowdtangle\n",
    "    reactions['user_desinformacion'] = user_desinformacion\n",
    "    reactions['link_user_desinformacion'] = link_user_desinformacion\n",
    "    \n",
    "    return(reactions)\n",
    "\n",
    "\n",
    "def obtain_panel_social(complete_fc):\n",
    "    data = pd.read_json('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/3-panel_disaggreagted/panel_facebook_posts.json')\n",
    "    data['date_publication'] = data['date_post'].apply(lambda x: clean_date_publication(x))\n",
    "    data['date_timestep'] = data['Score Date (GMT)'].apply(lambda x: clean_date_publication(x))\n",
    "    data['days_since_publication'] = data.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "    \n",
    "    social = pd.merge(left = data, \n",
    "        right = complete_fc.drop(['link_desinformacion'], axis = 1), \n",
    "        on = 'id_desinformacion', \n",
    "        how = 'left')\n",
    "    \n",
    "    social = social[~social['date_factcheck'].isna()]\n",
    "    \n",
    "    social['days_since_factcheck'] = social.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "    social['loveCount'] = pd.to_numeric(social.Loves)\n",
    "    social['hahaCount'] = pd.to_numeric(social.Wows)\n",
    "    social['wowCount'] = pd.to_numeric(social.Hahas)\n",
    "    social['sadCount'] = pd.to_numeric(social.Sads)\n",
    "    social['angryCount'] = pd.to_numeric(social.Angrys)\n",
    "    social['thankfulCount'] = pd.to_numeric(social.Cares)\n",
    "    \n",
    "    social['reactions'] = social.apply(lambda x: x['Loves'] + x['Wows'] + x['Hahas'] + x['Sads'] + x['Angrys'] + x['Cares'], axis = 1)\n",
    "    input_panel = social[['ID', 'id_desinformacion', 'date_publication', 'date_timestep', 'days_since_publication' ,'date_factcheck', 'days_since_factcheck',\n",
    "       'facebook_partnership_date', 'link_user_desinformacion', 'link_crowdtangle','organizacion', \n",
    "       'pais', 'id_factcheck','Likes', 'Comments', 'Shares', 'reactions']].rename(columns = {'ID':'id_post_desinformacion', \n",
    "                                             'link_crowdtangle':'link_desinformacion', \n",
    "                                                    'Likes':'likes', \n",
    "                                                    'Comments':'comments', \n",
    "                                                    'Shares':'shares'})\n",
    "    \n",
    "    \n",
    "    df_all = []\n",
    "    for i in tqdm(input_panel.id_post_desinformacion.unique()):\n",
    "        temp = input_panel[input_panel['id_post_desinformacion'] == i]\n",
    "        temp = temp.sort_values('days_since_publication')\n",
    "        temp = temp.drop_duplicates('days_since_publication', keep= 'last')\n",
    "        df_all.append(add_missing_timesteps_social(temp))\n",
    "        \n",
    "    df_final_aggregated = pd.concat(df_all)\n",
    "    panel_input = df_final_aggregated[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "       'days_since_publication', 'date_factcheck', 'days_since_factcheck',\n",
    "       'id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "       'facebook_partnership_date', 'organizacion', 'pais', 'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_interpolate = []\n",
    "    for i in tqdm(panel_input.id_post_desinformacion.unique()):\n",
    "        df = panel_input[panel_input['id_post_desinformacion'] == i].sort_values('days_since_publication')\n",
    "\n",
    "        df['approx_likes'] = df.likes.interpolate()\n",
    "        df['approx_shares'] = df.shares.interpolate()\n",
    "        df['approx_comments'] = df.comments.interpolate()\n",
    "        df['approx_reactions'] = df.reactions.interpolate()\n",
    "        panel_interpolate.append(df)   \n",
    "    \n",
    "    df_interpolate = pd.concat(panel_interpolate)\n",
    "    df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]\n",
    "    \n",
    "    df_interpolate['approx_reactions'] = pd.to_numeric(df_interpolate.approx_reactions)\n",
    "    df_interpolate['approx_likes'] = pd.to_numeric(df_interpolate.approx_likes)\n",
    "    df_interpolate['approx_shares'] = pd.to_numeric(df_interpolate.approx_shares)\n",
    "    df_interpolate['approx_comments'] = pd.to_numeric(df_interpolate.approx_comments)\n",
    "    df_interpolate['approx_interactions'] = df_interpolate.apply(lambda x: x['approx_likes'] + x['approx_comments'] + x['approx_shares'] + x['approx_reactions'], axis = 1)\n",
    "    \n",
    "    df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()\n",
    "    df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "    df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "    df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "    df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()\n",
    "    return(df_interpolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformation_media = load_misinformation_newspapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformation_social = load_social_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions- Engagements + Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "newspapers_link_checker.json\n"
     ]
    }
   ],
   "source": [
    "engagements_media = clean_engagements_media(misinformation_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36318/36318 [2:41:46<00:00,  3.74it/s]  \n"
     ]
    }
   ],
   "source": [
    "panel_media = make_panel_media(engagements_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    misinformation_newspapers_panel  = panel_media.sort_values(['id_post_desinformacion','days_since_publication']).reset_index()#.drop('rownames', axis = 1)\n",
    "    misinformation_newspapers_panel['loveCount'] = pd.to_numeric(misinformation_newspapers_panel.loveCount)\n",
    "    misinformation_newspapers_panel['hahaCount'] = pd.to_numeric(misinformation_newspapers_panel.hahaCount)\n",
    "    misinformation_newspapers_panel['wowCount'] = pd.to_numeric(misinformation_newspapers_panel.wowCount)\n",
    "    misinformation_newspapers_panel['sadCount'] = pd.to_numeric(misinformation_newspapers_panel.sadCount)\n",
    "    misinformation_newspapers_panel['angryCount'] = pd.to_numeric(misinformation_newspapers_panel.angryCount)\n",
    "    misinformation_newspapers_panel['thankfulCount'] = pd.to_numeric(misinformation_newspapers_panel.thankfulCount)\n",
    "    misinformation_newspapers_panel['careCount'] = pd.to_numeric(misinformation_newspapers_panel.careCount)\n",
    "    \n",
    "    misinformation_newspapers_panel['reactions'] = misinformation_newspapers_panel.apply(lambda x: x['loveCount'] + x['hahaCount'] + x['wowCount'] + x['sadCount'] + x['angryCount'] + x['thankfulCount'] + x['careCount'], axis = 1)\n",
    "    misinformation_newspapers_panel['likes'] = pd.to_numeric(misinformation_newspapers_panel.likeCount)\n",
    "    misinformation_newspapers_panel['shares'] = pd.to_numeric(misinformation_newspapers_panel.shareCount)\n",
    "    misinformation_newspapers_panel['comments'] = pd.to_numeric(misinformation_newspapers_panel.commentCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35708 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'likes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-8ebae8528d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_media\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpolate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpanel_media\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-126-6c1e7a8323dc>\u001b[0m in \u001b[0;36minterpolate_function\u001b[0;34m(panel)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpanel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_post_desinformacion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpanel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpanel_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_post_desinformacion'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'days_since_publication'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'approx_likes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'approx_shares'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshares\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'approx_comments'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5459\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5460\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'likes'"
     ]
    }
   ],
   "source": [
    "final_media = interpolate_function(panel_media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:08<00:00, 13.54it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 284.23it/s]\n"
     ]
    }
   ],
   "source": [
    "panel_social = obtain_panel_social(misinformation_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
