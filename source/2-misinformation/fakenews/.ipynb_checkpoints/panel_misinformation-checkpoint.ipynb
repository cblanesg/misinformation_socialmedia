{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "import uuid\n",
    "from datetime import date\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_factcheck(x, y, z):\n",
    "    return(datetime.date(x, y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(misinformation_social):\n",
    "    newspapers = load_misinformation_newspapers()[['id_desinformacion', 'label_desinformacion']].drop_duplicates(['id_desinformacion'], keep = 'first')\n",
    "    social = misinformation_social[['id_desinformacion', 'label_desinformacion']].drop_duplicates(['id_desinformacion'], keep = 'first')\n",
    "    return(pd.concat([newspapers, social]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_africa_data():\n",
    "    africa = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/input_panel_africa.xlsx')\n",
    "    links = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/fact_checks/fact_checks_clean.xlsx')[['Link', \n",
    "'id_desinformacion']].drop_duplicates().rename(columns = {'Link':'link_desinformacion'})\n",
    "    all_data = pd.merge(left = africa, \n",
    "            right = links, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left')\n",
    "    return(all_data)\n",
    "\n",
    "def load_factchecks():\n",
    "    factchecks = pd.read_excel('../../../data/1-factchecks/2-clean_factchecks/subset_factchecks_data.xlsx')\n",
    "    factchecks['date_factcheck'] = factchecks.apply(lambda x: datetime.date(x['year'],\n",
    "                                                                            x['month'], \n",
    "                                                                            x['day']), axis = 1)\n",
    "    return(factchecks)\n",
    "\n",
    "def dates_function(x):\n",
    "    try:\n",
    "        if pd.isna(x) or  type(x) is str:\n",
    "            return(None)\n",
    "        else:\n",
    "            return(x.strftime('%Y-%m-%d'))\n",
    "    except:\n",
    "        return(x)\n",
    "\n",
    "def load_misinformation_newspapers():\n",
    "    path = '/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/04-fake_news_dataset/newspapers/Fact-Checkers/'\n",
    "    data = []\n",
    "    for i in listdir(path):\n",
    "        try:\n",
    "            data.append(pd.read_excel(path + i))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    misinformation = pd.concat(data)\n",
    "    misinformation = misinformation[[ 'id_desinformacion', 'fecha_desinformacion', \n",
    "               'link_desinformacion',  'label_desinformacion', \n",
    "                                    'id_chequeo']].drop_duplicates().rename(columns = {'id_chequeo':'id_factcheck'})\n",
    "    \n",
    "    ## include africa\n",
    "    africa = load_africa_data()\n",
    "    africa['fecha_desinformacion'] = None\n",
    "    africa = africa[list(misinformation.columns)]\n",
    "    \n",
    "    all_data = pd.concat([misinformation, africa])\n",
    "    all_data = all_data[~all_data['link_desinformacion'].isna()]\n",
    "    \n",
    "    factchecks = load_factchecks().drop(['Unnamed: 0'], axis = 1).rename(columns = {'link_facebook_factcheck':'link_post_factcheck', \n",
    "                                                                                   'date_factcheck_facebook':'date_post_factcheck'})\n",
    "    factchecks = factchecks[factchecks['type_factcheck'] == 'medios_tradicionales']\n",
    "    all_data = pd.merge(left = all_data, \n",
    "            right = factchecks, \n",
    "            on = 'id_factcheck', \n",
    "            how = 'left')\n",
    "    \n",
    "    all_data = all_data[~all_data['id_factcheck'].isna()]\n",
    "    all_data = all_data[~all_data['label_desinformacion'].isna()]\n",
    "    all_data['label_desinformacion'] = np.where(all_data['label_desinformacion'] == 'true ', 'true', all_data['label_desinformacion'])\n",
    "    all_data = all_data.drop(['fecha_desinformacion'], axis = 1)\n",
    "    return(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_africa_data_social():\n",
    "    africa = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/africa/social/0-prepare_data/facebook_africa.xlsx')[['clean_link', \n",
    "    'id_post', \n",
    "    'id_desinformacion', \n",
    "    'Verdict']].rename(columns = {'clean_link':'link_desinformacion'})\n",
    "    africa['link_post'] = africa.link_desinformacion\n",
    "    africa['label_desinformacion'] = africa['Verdict'].apply(lambda x: clean_label_social(x))\n",
    "    return(africa)\n",
    "\n",
    "def clean_label_social(x):\n",
    "    if x in ['Incorrect','Understated, exaggerated and incorrect',\n",
    "            'Seven incorrect, three correct, three unproven and one mostly correct']:\n",
    "        return('fake')\n",
    "    elif x in ['Mostly correct',\n",
    "               'A range of verdicts ranging from misleading to unproven and correct',\n",
    "              'Checked', 'Misleading']:\n",
    "        return('misleading')\n",
    "    else:\n",
    "        return('true')\n",
    "\n",
    "def load_social_data():\n",
    "    social = pd.read_excel('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/2-id_data/virality_fake_posts.xlsx')\n",
    "\n",
    "    social_misinformation = social[['id_post', 'id_desinformacion',  'link_crowdtangle', \n",
    "       'label_desinformacion']].rename(columns = {'link_crowdtangle':'link_desinformacion'})\n",
    "    \n",
    "    africa = load_africa_data_social()\n",
    "    africa = africa[list(social_misinformation.columns)]\n",
    "    all_social = pd.concat([social_misinformation, africa])\n",
    "    \n",
    "    factchecks = load_factchecks().drop(['Unnamed: 0'], axis = 1).rename(columns = {'link_facebook_factcheck':'link_post_factcheck', \n",
    "                                                                                   'date_factcheck_facebook':'date_post_factcheck'})\n",
    "    factchecks = factchecks[factchecks['type_factcheck'] == 'social_media']\n",
    "    \n",
    "    social_id = pd.read_excel('../../../data/1-factchecks/2-clean_factchecks/factchecks_subset_social.xlsx').dropna()\n",
    "    social_id = social_id[['id_desinformacion', 'id_factcheck']].drop_duplicates()\n",
    "    \n",
    "    factchecks = pd.merge(left = factchecks, \n",
    "            right = social_id, \n",
    "            on = 'id_factcheck', how = 'left')\n",
    "    factchecks = factchecks[~factchecks['id_desinformacion'].isna()]\n",
    "    \n",
    "    all_data = pd.merge(left = all_social, \n",
    "            right = factchecks, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left').drop_duplicates()\n",
    "    \n",
    "    \n",
    "    all_data = all_data[~all_data['id_factcheck'].isna()]\n",
    "    return(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions Engagements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engagements_media():\n",
    "    path = '../../../data/2-misinformation/newspapers/1-engagements/1-scrapes/'\n",
    "    data = []\n",
    "    for i in listdir(path):\n",
    "        try:\n",
    "            data.append(pd.read_json(path + i))\n",
    "        except:\n",
    "            print(i)\n",
    "    data = pd.concat(data)\n",
    "    return(data)\n",
    "\n",
    "def clean_engagements_media(misinformations):\n",
    "    engagements = load_engagements_media()\n",
    "    \n",
    "    engagements['date_publication'] = engagements['date'].apply(lambda x: x.date())\n",
    "    engagements = engagements[['id_desinformacion',\n",
    "                               'id',\n",
    "                               'date_publication',\n",
    "                               'history',\n",
    "                               'postUrl']].rename(columns = {'id':'id_post_desinformacion', \n",
    "                                             'postUrl':'link_post_desinformacion'})\n",
    "    \n",
    "    all_data = pd.merge(left = engagements, \n",
    "            right = misinformations, \n",
    "            on = 'id_desinformacion', \n",
    "            how = 'left')\n",
    "    return(all_data)\n",
    "\n",
    "def add_missing_timesteps_media(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row =[np.nan, \n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan,\n",
    "             np.nan]\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            row.append(timestep)\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)\n",
    "\n",
    "\n",
    "def make_panel_media(engagements):\n",
    "    engagements = engagements[~engagements['date_factcheck'].isna()]\n",
    "    engagements = engagements.reset_index().drop('index', axis = 1)\n",
    "    \n",
    "    df_all = []\n",
    "\n",
    "    for i in tqdm(range(0, len(engagements))):\n",
    "        try:\n",
    "            post = engagements.loc[i]\n",
    "            if pd.isna(post['date_factcheck']):\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                df_history = pd.DataFrame(post['history'])\n",
    "                df_history['date_timestep'] = df_history['date'].apply(lambda x: datetime.strptime(x.split(' ')[0], '%Y-%m-%d').date())\n",
    "                df_history = df_history.reset_index().drop('index', axis = 1)\n",
    "\n",
    "                df_eng_post = []\n",
    "                for h in range(0, len(df_history)):\n",
    "                    columns = list(df_history.loc[h]['actual'].keys())\n",
    "                    values = list(df_history.loc[h]['actual'].values())\n",
    "                    df_temp = pd.DataFrame([values], columns = columns)\n",
    "                    df_temp['date_timestep'] = df_history.loc[h]['date_timestep']\n",
    "                    df_temp['id_desinformacion'] = engagements.loc[i]['id_desinformacion']\n",
    "                    df_temp = df_temp.drop_duplicates('date_timestep', keep = 'first')\n",
    "                    df_temp['date_publication'] = post['date_publication']\n",
    "                    df_temp['days_since_publication'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "                    df_temp['date_factcheck'] = post['date_factcheck']\n",
    "                    df_temp['days_since_factcheck'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "                    df_temp['id_post_desinformacion'] = post['id_post_desinformacion']\n",
    "                    df_temp['id_factcheck'] = post['id_factcheck']\n",
    "                    df_temp['link_desinformacion'] = post['link_desinformacion']\n",
    "                    df_temp['facebook_partnership_date'] = post['facebook_partnership_date']\n",
    "                    df_temp['organizacion'] = post['organizacion']\n",
    "                    df_temp['pais'] = post['pais']\n",
    "                    df_eng_post.append(df_temp)\n",
    "                temp = pd.concat(df_eng_post).drop_duplicates('days_since_publication', keep = 'first')\n",
    "                panel = add_missing_timesteps_media(temp)\n",
    "                df_all.append(panel)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    df_final_aggregated = pd.concat(df_all)\n",
    "    return(df_final_aggregated)\n",
    "\n",
    "\n",
    "def interpolate_function(panel):\n",
    "    misinformation_newspapers_panel  = panel.sort_values(['id_post_desinformacion','days_since_publication']).reset_index()#.drop('rownames', axis = 1)\n",
    "    misinformation_newspapers_panel['loveCount'] = pd.to_numeric(misinformation_newspapers_panel.loveCount)\n",
    "    misinformation_newspapers_panel['hahaCount'] = pd.to_numeric(misinformation_newspapers_panel.hahaCount)\n",
    "    misinformation_newspapers_panel['wowCount'] = pd.to_numeric(misinformation_newspapers_panel.wowCount)\n",
    "    misinformation_newspapers_panel['sadCount'] = pd.to_numeric(misinformation_newspapers_panel.sadCount)\n",
    "    misinformation_newspapers_panel['angryCount'] = pd.to_numeric(misinformation_newspapers_panel.angryCount)\n",
    "    misinformation_newspapers_panel['thankfulCount'] = pd.to_numeric(misinformation_newspapers_panel.thankfulCount)\n",
    "    misinformation_newspapers_panel['careCount'] = pd.to_numeric(misinformation_newspapers_panel.careCount)\n",
    "    \n",
    "    misinformation_newspapers_panel['reactions'] = misinformation_newspapers_panel.apply(lambda x: x['loveCount'] + x['hahaCount'] + x['wowCount'] + x['sadCount'] + x['angryCount'] + x['thankfulCount'] + x['careCount'], axis = 1)\n",
    "    misinformation_newspapers_panel['likes'] = pd.to_numeric(misinformation_newspapers_panel.likeCount)\n",
    "    misinformation_newspapers_panel['shares'] = pd.to_numeric(misinformation_newspapers_panel.shareCount)\n",
    "    misinformation_newspapers_panel['comments'] = pd.to_numeric(misinformation_newspapers_panel.commentCount)\n",
    "    \n",
    "    panel_input = misinformation_newspapers_panel[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "                                                   'days_since_publication', 'date_factcheck', 'days_since_factcheck','id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "                                                   'facebook_partnership_date', 'organizacion', 'pais', \n",
    "                                                   'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_input = panel_input.sort_values(['id_post_desinformacion','days_since_publication'])\n",
    "    \n",
    "    \n",
    "    panel_interpolate = []\n",
    "    for i in tqdm(panel_input.id_post_desinformacion.unique()):\n",
    "        df = panel_input[panel_input['id_post_desinformacion'] == i].sort_values('days_since_publication')\n",
    "        df['approx_likes'] = df.likes.interpolate()\n",
    "        df['approx_shares'] = df.shares.interpolate()\n",
    "        df['approx_comments'] = df.comments.interpolate()\n",
    "        df['approx_reactions'] = df.reactions.interpolate()\n",
    "        df['approx_interactions'] = df.apply(lambda x: x['approx_likes'] + x['approx_shares'] + x['approx_comments'] + x['approx_reactions'], axis = 1)\n",
    "        panel_interpolate.append(df)   \n",
    "        \n",
    "    df_interpolate = pd.concat(panel_interpolate)\n",
    "    df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]\n",
    "    \n",
    "    \n",
    "    df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "    df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "    df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()\n",
    "    df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "    df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()\n",
    "    return(df_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_timesteps_social(temp):\n",
    "    missing_rows = []\n",
    "    for i in list(range(-16, 16)):\n",
    "        if i not in temp.days_since_factcheck.unique():\n",
    "            row = []\n",
    "            timestep = list(temp.date_factcheck)[0] + timedelta(i)\n",
    "            \n",
    "            \n",
    "            row.append(list(temp.id_post_desinformacion)[0])\n",
    "            row.append(list(temp.id_desinformacion)[0])\n",
    "            row.append(list(temp.date_publication)[0])\n",
    "            row.append(timestep)\n",
    "            row.append((timestep - list(temp.date_publication)[0]).days)\n",
    "            row.append(list(temp.date_factcheck)[0])\n",
    "            row.append(i)\n",
    "            row.append(list(temp.facebook_partnership_date)[0])\n",
    "            row.append(list(temp.link_user_desinformacion)[0])\n",
    "            row.append(list(temp.link_desinformacion)[0])\n",
    "            row.append(list(temp.organizacion)[0])\n",
    "            row.append(list(temp.pais)[0])\n",
    "            row.append(list(temp.id_factcheck)[0])\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            row.append(np.nan)\n",
    "            \n",
    "            m_row = pd.DataFrame([row], columns = list(temp.columns))\n",
    "            missing_rows.append(m_row)\n",
    "    temp_clean = pd.concat([temp, pd.concat(missing_rows)])\n",
    "    temp_clean = temp_clean[temp_clean['days_since_publication'] >= 0]\n",
    "    return(temp_clean)\n",
    "\n",
    "def changeid(link, id_post):\n",
    "    if any(re.findall('permalink[.]php[?]story', link)):\n",
    "        id_clean = link.split('story_fbid=')[1].split('&id=')[0]\n",
    "        return(id_clean)\n",
    "    else:\n",
    "        return(id_post)\n",
    "    \n",
    "def clean_date_publication(x):\n",
    "    str_date = x.split(' ')[0]\n",
    "    return(datetime.date(int(x.split(' ')[0].split('-')[0]), \n",
    "                         int(x.split(' ')[0].split('-')[1]),\n",
    "                        int(x.split(' ')[0].split('-')[2])))    \n",
    "\n",
    "\n",
    "def obtian_statistics_virality(df_crowd):\n",
    "    date_post = df_crowd.columns[-2:][0]\n",
    "    \n",
    "    reactions = df_crowd[['ID', 'Score Date (GMT)', 'Timestep', \n",
    "               'Likes', 'Comments', 'Shares', 'Loves', 'Wows', 'Hahas', 'Sads', 'Angrys', 'Cares', 'Reactions', 'Total Views', 'Overperforming(+)/Underperforming(-) Score']]\n",
    "    \n",
    "    link_crowdtangle = df_crowd.columns[-1:][0]\n",
    "    user_desinformacion = re.sub('https://www.facebook.com/', '', link_crowdtangle).split('/')[0]\n",
    "    link_user_desinformacion = 'https://www.facebook.com/' + user_desinformacion\n",
    "    \n",
    "    reactions['date_post'] = date_post\n",
    "    reactions['link_crowdtangle'] = link_crowdtangle\n",
    "    reactions['user_desinformacion'] = user_desinformacion\n",
    "    reactions['link_user_desinformacion'] = link_user_desinformacion\n",
    "    \n",
    "    return(reactions)\n",
    "\n",
    "\n",
    "def obtain_panel_social(complete_fc):\n",
    "    data = pd.read_json('/Users/cblanesg/cam.blanes Dropbox/Camila Blanes/Bolivia_Project/data/04-fakenews-Repository/06-virality/social/3-panel_disaggreagted/panel_facebook_posts.json')\n",
    "    data['date_publication'] = data['date_post'].apply(lambda x: clean_date_publication(x))\n",
    "    data['date_timestep'] = data['Score Date (GMT)'].apply(lambda x: clean_date_publication(x))\n",
    "    data['days_since_publication'] = data.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "    \n",
    "    social = pd.merge(left = data, \n",
    "        right = complete_fc.drop(['link_desinformacion'], axis = 1), \n",
    "        on = 'id_desinformacion', \n",
    "        how = 'left')\n",
    "    \n",
    "    social = social[~social['date_factcheck'].isna()]\n",
    "    \n",
    "    social['days_since_factcheck'] = social.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "    social['loveCount'] = pd.to_numeric(social.Loves)\n",
    "    social['hahaCount'] = pd.to_numeric(social.Wows)\n",
    "    social['wowCount'] = pd.to_numeric(social.Hahas)\n",
    "    social['sadCount'] = pd.to_numeric(social.Sads)\n",
    "    social['angryCount'] = pd.to_numeric(social.Angrys)\n",
    "    social['thankfulCount'] = pd.to_numeric(social.Cares)\n",
    "    \n",
    "    social['reactions'] = social.apply(lambda x: x['Loves'] + x['Wows'] + x['Hahas'] + x['Sads'] + x['Angrys'] + x['Cares'], axis = 1)\n",
    "    input_panel = social[['ID', 'id_desinformacion', 'date_publication', 'date_timestep', 'days_since_publication' ,'date_factcheck', 'days_since_factcheck',\n",
    "       'facebook_partnership_date', 'link_user_desinformacion', 'link_crowdtangle','organizacion', \n",
    "       'pais', 'id_factcheck','Likes', 'Comments', 'Shares', 'reactions']].rename(columns = {'ID':'id_post_desinformacion', \n",
    "                                             'link_crowdtangle':'link_desinformacion', \n",
    "                                                    'Likes':'likes', \n",
    "                                                    'Comments':'comments', \n",
    "                                                    'Shares':'shares'})\n",
    "    \n",
    "    \n",
    "    df_all = []\n",
    "    for i in tqdm(input_panel.id_post_desinformacion.unique()):\n",
    "        temp = input_panel[input_panel['id_post_desinformacion'] == i]\n",
    "        temp = temp.sort_values('days_since_publication')\n",
    "        temp = temp.drop_duplicates('days_since_publication', keep= 'last')\n",
    "        df_all.append(add_missing_timesteps_social(temp))\n",
    "        \n",
    "    df_final_aggregated = pd.concat(df_all)\n",
    "    panel_input = df_final_aggregated[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "       'days_since_publication', 'date_factcheck', 'days_since_factcheck',\n",
    "       'id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "       'facebook_partnership_date', 'organizacion', 'pais', 'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_interpolate = []\n",
    "    for i in tqdm(panel_input.id_post_desinformacion.unique()):\n",
    "        df = panel_input[panel_input['id_post_desinformacion'] == i].sort_values('days_since_publication')\n",
    "\n",
    "        df['approx_likes'] = df.likes.interpolate()\n",
    "        df['approx_shares'] = df.shares.interpolate()\n",
    "        df['approx_comments'] = df.comments.interpolate()\n",
    "        df['approx_reactions'] = df.reactions.interpolate()\n",
    "        panel_interpolate.append(df)   \n",
    "    \n",
    "    df_interpolate = pd.concat(panel_interpolate)\n",
    "    df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]\n",
    "    \n",
    "    df_interpolate['approx_reactions'] = pd.to_numeric(df_interpolate.approx_reactions)\n",
    "    df_interpolate['approx_likes'] = pd.to_numeric(df_interpolate.approx_likes)\n",
    "    df_interpolate['approx_shares'] = pd.to_numeric(df_interpolate.approx_shares)\n",
    "    df_interpolate['approx_comments'] = pd.to_numeric(df_interpolate.approx_comments)\n",
    "    df_interpolate['approx_interactions'] = df_interpolate.apply(lambda x: x['approx_likes'] + x['approx_comments'] + x['approx_shares'] + x['approx_reactions'], axis = 1)\n",
    "    \n",
    "    df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()\n",
    "    df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "    df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "    df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "    df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()\n",
    "    return(df_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def make_panel_media(engagements):\n",
    "        engagements = engagements[~engagements['date_factcheck'].isna()]\n",
    "        engagements = engagements.reset_index().drop('index', axis = 1)\n",
    "\n",
    "        df_all = []\n",
    "\n",
    "        for i in tqdm(range(0, len(engagements))):\n",
    "            post = engagements.loc[i]\n",
    "            \n",
    "            if pd.isna(post['date_factcheck']):\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                df_history = pd.DataFrame(post['history'])\n",
    "                df_history['date_timestep'] = df_history['date'].apply(lambda x: datetime.strptime(x.split(' ')[0], '%Y-%m-%d').date())\n",
    "                df_history = df_history.reset_index().drop('index', axis = 1)\n",
    "\n",
    "                df_eng_post = []\n",
    "                for h in range(0, len(df_history)):\n",
    "                    columns = list(df_history.loc[h]['actual'].keys())\n",
    "                    values = list(df_history.loc[h]['actual'].values())\n",
    "                    df_temp = pd.DataFrame([values], columns = columns)\n",
    "                    df_temp['date_timestep'] = df_history.loc[h]['date_timestep']\n",
    "                    df_temp['id_desinformacion'] = engagements.loc[i]['id_desinformacion']\n",
    "                    df_temp = df_temp.drop_duplicates('date_timestep', keep = 'first')\n",
    "                    df_temp['date_publication'] = post['date_publication']\n",
    "                    df_temp['days_since_publication'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_publication']).days, axis = 1)\n",
    "                    df_temp['date_factcheck'] = post['date_factcheck']\n",
    "                    df_temp['days_since_factcheck'] = df_temp.apply(lambda x: (x['date_timestep'] - x['date_factcheck']).days, axis = 1)\n",
    "                    df_temp['id_post_desinformacion'] = post['id_post_desinformacion']\n",
    "                    df_temp['id_factcheck'] = post['id_factcheck']\n",
    "                    df_temp['link_desinformacion'] = post['link_desinformacion']\n",
    "                    df_temp['facebook_partnership_date'] = post['facebook_partnership_date']\n",
    "                    df_temp['organizacion'] = post['organizacion']\n",
    "                    df_temp['pais'] = post['pais']\n",
    "                    df_eng_post.append(df_temp)\n",
    "                temp = pd.concat(df_eng_post).drop_duplicates('days_since_publication', keep = 'first')\n",
    "                panel = add_missing_timesteps_media(temp)\n",
    "                df_all.append(panel)\n",
    "       \n",
    "            \n",
    "        df_final_aggregated = pd.concat(df_all)\n",
    "        return(df_final_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformation_media = load_misinformation_newspapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformation_social = load_social_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Functions- Engagements + Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "newspapers_link_checker.json\n"
     ]
    }
   ],
   "source": [
    "engagements_media = clean_engagements_media(misinformation_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36318/36318 [2:52:47<00:00,  3.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "panel_media = make_panel_media(engagements_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_function(panel):\n",
    "    misinformation_newspapers_panel  = panel.sort_values(['id_post_desinformacion','days_since_publication']).reset_index()#.drop('rownames', axis = 1)\n",
    "    misinformation_newspapers_panel['loveCount'] = pd.to_numeric(misinformation_newspapers_panel.loveCount)\n",
    "    misinformation_newspapers_panel['hahaCount'] = pd.to_numeric(misinformation_newspapers_panel.hahaCount)\n",
    "    misinformation_newspapers_panel['wowCount'] = pd.to_numeric(misinformation_newspapers_panel.wowCount)\n",
    "    misinformation_newspapers_panel['sadCount'] = pd.to_numeric(misinformation_newspapers_panel.sadCount)\n",
    "    misinformation_newspapers_panel['angryCount'] = pd.to_numeric(misinformation_newspapers_panel.angryCount)\n",
    "    misinformation_newspapers_panel['thankfulCount'] = pd.to_numeric(misinformation_newspapers_panel.thankfulCount)\n",
    "    misinformation_newspapers_panel['careCount'] = pd.to_numeric(misinformation_newspapers_panel.careCount)\n",
    "    \n",
    "    misinformation_newspapers_panel['reactions'] = misinformation_newspapers_panel.apply(lambda x: x['loveCount'] + x['hahaCount'] + x['wowCount'] + x['sadCount'] + x['angryCount'] + x['thankfulCount'] + x['careCount'], axis = 1)\n",
    "    misinformation_newspapers_panel['likes'] = pd.to_numeric(misinformation_newspapers_panel.likeCount)\n",
    "    misinformation_newspapers_panel['shares'] = pd.to_numeric(misinformation_newspapers_panel.shareCount)\n",
    "    misinformation_newspapers_panel['comments'] = pd.to_numeric(misinformation_newspapers_panel.commentCount)\n",
    "    \n",
    "    panel_input = misinformation_newspapers_panel[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "                                                   'days_since_publication', 'date_factcheck', 'days_since_factcheck','id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "                                                   'facebook_partnership_date', 'organizacion', 'pais', \n",
    "                                                   'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_input = panel_input.sort_values(['id_post_desinformacion','days_since_publication'])\n",
    "    panel_input['id_factcheck_post_misinf'] = panel_input.apply(lambda x: x['id_factcheck'] + '_' + x['id_post_desinformacion'], axis = 1)\n",
    "    \n",
    "    panel_interpolate = []\n",
    "    for i in tqdm(panel_input.id_factcheck_post_misinf.unique()):\n",
    "        df = panel_input[panel_input['id_factcheck_post_misinf'] == i].sort_values('days_since_publication')\n",
    "        df['approx_likes'] = df.likes.interpolate()\n",
    "        df['approx_shares'] = df.shares.interpolate()\n",
    "        df['approx_comments'] = df.comments.interpolate()\n",
    "        df['approx_reactions'] = df.reactions.interpolate()\n",
    "        df['approx_interactions'] = df.apply(lambda x: x['approx_likes'] + x['approx_shares'] + x['approx_comments'] + x['approx_reactions'], axis = 1)\n",
    "        panel_interpolate.append(df)   \n",
    "        \n",
    "    df_interpolate = pd.concat(panel_interpolate)\n",
    "    df_interpolate = df_interpolate[abs(df_interpolate['days_since_factcheck']) <=15]\n",
    "    \n",
    "    \n",
    "    df_interpolate['growth_likes'] = df_interpolate.groupby(['id_post_desinformacion']).approx_likes.pct_change()\n",
    "    df_interpolate['growth_shares'] = df_interpolate.groupby(['id_post_desinformacion']).approx_shares.pct_change()\n",
    "    df_interpolate['growth_reactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_reactions.pct_change()\n",
    "    df_interpolate['growth_comments'] = df_interpolate.groupby(['id_post_desinformacion']).approx_comments.pct_change()\n",
    "    df_interpolate['growth_interactions'] = df_interpolate.groupby(['id_post_desinformacion']).approx_interactions.pct_change()\n",
    "    return(df_interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = panel_media "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    misinformation_newspapers_panel  = panel.sort_values(['id_post_desinformacion','days_since_publication']).reset_index()#.drop('rownames', axis = 1)\n",
    "    misinformation_newspapers_panel['loveCount'] = pd.to_numeric(misinformation_newspapers_panel.loveCount)\n",
    "    misinformation_newspapers_panel['hahaCount'] = pd.to_numeric(misinformation_newspapers_panel.hahaCount)\n",
    "    misinformation_newspapers_panel['wowCount'] = pd.to_numeric(misinformation_newspapers_panel.wowCount)\n",
    "    misinformation_newspapers_panel['sadCount'] = pd.to_numeric(misinformation_newspapers_panel.sadCount)\n",
    "    misinformation_newspapers_panel['angryCount'] = pd.to_numeric(misinformation_newspapers_panel.angryCount)\n",
    "    misinformation_newspapers_panel['thankfulCount'] = pd.to_numeric(misinformation_newspapers_panel.thankfulCount)\n",
    "    misinformation_newspapers_panel['careCount'] = pd.to_numeric(misinformation_newspapers_panel.careCount)\n",
    "    \n",
    "    misinformation_newspapers_panel['reactions'] = misinformation_newspapers_panel.apply(lambda x: x['loveCount'] + x['hahaCount'] + x['wowCount'] + x['sadCount'] + x['angryCount'] + x['thankfulCount'] + x['careCount'], axis = 1)\n",
    "    misinformation_newspapers_panel['likes'] = pd.to_numeric(misinformation_newspapers_panel.likeCount)\n",
    "    misinformation_newspapers_panel['shares'] = pd.to_numeric(misinformation_newspapers_panel.shareCount)\n",
    "    misinformation_newspapers_panel['comments'] = pd.to_numeric(misinformation_newspapers_panel.commentCount)\n",
    "    \n",
    "    panel_input = misinformation_newspapers_panel[['date_timestep', 'id_desinformacion', 'date_publication',\n",
    "                                                   'days_since_publication', 'date_factcheck', 'days_since_factcheck','id_post_desinformacion', 'id_factcheck', 'link_desinformacion',\n",
    "                                                   'facebook_partnership_date', 'organizacion', 'pais', \n",
    "                                                   'likes', 'shares', 'comments', 'reactions']]\n",
    "    \n",
    "    panel_input = panel_input.sort_values(['id_factcheck','id_post_desinformacion','days_since_publication'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = panel_input[['days_since_publication','id_factcheck','id_post_desinformacion', 'likes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.set_index('days_since_publication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_factcheck</th>\n",
       "      <th>id_post_desinformacion</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_since_publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>00259a1a-e5c8-3457-a3e8-68bfff774a7f</td>\n",
       "      <td>10139921|2686076924763795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>00259a1a-e5c8-3457-a3e8-68bfff774a7f</td>\n",
       "      <td>10139921|2686076924763795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>00259a1a-e5c8-3457-a3e8-68bfff774a7f</td>\n",
       "      <td>10139921|2686076924763795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>00259a1a-e5c8-3457-a3e8-68bfff774a7f</td>\n",
       "      <td>10139921|2686076924763795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00259a1a-e5c8-3457-a3e8-68bfff774a7f</td>\n",
       "      <td>10139921|2686076924763795</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ffd2a8bf-5e00-38b7-8470-405c9c376c20</td>\n",
       "      <td>9000816|1511326365666813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ffd2a8bf-5e00-38b7-8470-405c9c376c20</td>\n",
       "      <td>9000816|1511326365666813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ffd2a8bf-5e00-38b7-8470-405c9c376c20</td>\n",
       "      <td>9000816|1511326365666813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ffd2a8bf-5e00-38b7-8470-405c9c376c20</td>\n",
       "      <td>9000816|1511326365666813</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>ffd2a8bf-5e00-38b7-8470-405c9c376c20</td>\n",
       "      <td>9000816|1511326365666813</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>838342 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                id_factcheck  \\\n",
       "days_since_publication                                         \n",
       "25                      00259a1a-e5c8-3457-a3e8-68bfff774a7f   \n",
       "26                      00259a1a-e5c8-3457-a3e8-68bfff774a7f   \n",
       "27                      00259a1a-e5c8-3457-a3e8-68bfff774a7f   \n",
       "28                      00259a1a-e5c8-3457-a3e8-68bfff774a7f   \n",
       "29                      00259a1a-e5c8-3457-a3e8-68bfff774a7f   \n",
       "...                                                      ...   \n",
       "12                      ffd2a8bf-5e00-38b7-8470-405c9c376c20   \n",
       "13                      ffd2a8bf-5e00-38b7-8470-405c9c376c20   \n",
       "14                      ffd2a8bf-5e00-38b7-8470-405c9c376c20   \n",
       "15                      ffd2a8bf-5e00-38b7-8470-405c9c376c20   \n",
       "578                     ffd2a8bf-5e00-38b7-8470-405c9c376c20   \n",
       "\n",
       "                           id_post_desinformacion  likes  \n",
       "days_since_publication                                    \n",
       "25                      10139921|2686076924763795    NaN  \n",
       "26                      10139921|2686076924763795    NaN  \n",
       "27                      10139921|2686076924763795    NaN  \n",
       "28                      10139921|2686076924763795    NaN  \n",
       "29                      10139921|2686076924763795    NaN  \n",
       "...                                           ...    ...  \n",
       "12                       9000816|1511326365666813    NaN  \n",
       "13                       9000816|1511326365666813    NaN  \n",
       "14                       9000816|1511326365666813    NaN  \n",
       "15                       9000816|1511326365666813    NaN  \n",
       "578                      9000816|1511326365666813    1.0  \n",
       "\n",
       "[838342 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupby(['id_factcheck','id_post_desinformacion']).apply(lambda group: group.interpolate(method='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['approx_likes'] = df.likes.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35708/35708 [31:45<00:00, 18.74it/s]   \n"
     ]
    }
   ],
   "source": [
    "final_media = interpolate_function(panel_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_media_clean = final_media.drop_duplicates(['id_post_desinformacion', \n",
    "                                                 'days_since_publication'], keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:08<00:00, 13.54it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 284.23it/s]\n"
     ]
    }
   ],
   "source": [
    "panel_social = obtain_panel_social(misinformation_social)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_social = panel_social[final_media.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_social['type_misinformation'] = 'social_media'\n",
    "final_media_clean['type_misinformation'] = 'media_outlets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformation_data = pd.concat([panel_social, \n",
    "                                final_media_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = load_labels(misinformation_social)\n",
    "misinformation_data = pd.merge(left = misinformation_data, \n",
    "        right = labels, \n",
    "        on = 'id_desinformacion', \n",
    "        how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyreadr.write_rdata('../../../data/4-panel_data/misinformation/panel_interpolate.RData', misinformation_data, df_name=\"misinformation_panel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
